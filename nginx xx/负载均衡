Nginx负载均衡配置语法：

一、基本配置语法：
语法：

Syntax：upstream name {…}
Default:—
Context：http
注意：配置项的位置，必须再server层以外
首先我有2台服务器：

IP地址分别为：

192.168.37.129(被负载的服务器) A服务器
192.168.65.197 (客户端访问这个IP地址)B服务器
首先再A服务器上面,创建配置文件：

创建第一个配置项, server1.conf：

server {
    listen       8001;
    server_name  localhost;

    #charset koi8-r;
    access_log  /var/log/nginx/log/server1.access.log;

    location / {
        root   /opt/app/code1;   # 路径下面有对应的文件 index.html 
        index  index.html index.htm;
    }
}

再创建第二个配置项， server2.conf

server {
    listen       8002;
    server_name  localhost;

    #charset koi8-r;
    access_log  /var/log/nginx/log/server1.access.log;

    location / {
        root   /opt/app/code2;   # 路径下面有对应的文件 index.html 
        index  index.html index.htm;
    }
}

再创建第三个配置项，server3.conf

server {
    listen       8003;
    server_name  localhost;

    #charset koi8-r;
    access_log  /var/log/nginx/log/server1.access.log;

    location / {
        root   /opt/app/code3;   # 路径下面有对应的文件 index.html 
        index  index.html index.htm;
    }
}

会发现就是端口不一样， 访问的静态资源路径也是不一样的

再B服务器上修改Nginx配置项：

打开nginx.conf 主配置文件：

# 首次按tomcat_server 必须跟下面的 location / {} 里面的 proxy_pass http:// `tomcat_server` 名字是一样的
upstream tomcat_server {
        #设定负载均衡的服务器列表
              server 192.168.37.129:8001;  # 就是A服务器上面的启动的8001端口

              server 192.168.37.129:8002;  # 就是A服务器上面的启动的8002端口

              server 192.168.37.129:8003; # 就是A服务器上面的启动的8003端口
        }
    server {
        listen       80;   # B服务器的端口
        server_name  localhost;  # B服务器的IP地址

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location / {
            proxy_buffering off;
            proxy_pass http://tomcat_server;
            root   html;
            index  index.html index.htm;
        }

        #error_page  404              /404.html;

        # redirect server error pages to the static page /50x.html
        #
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
}

访问过程：

首先配置完Nginx的配置项：

第一步.nginx -t : 检查Nginx 配置文件是否正确：

如果出现, 那恭喜你配置成功：

nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
nginx: configuration file /etc/nginx/nginx.conf test is successful

如果出现报错信息, 去相对应的行号寻找就好。

第二步.nginx -s reload : 平缓的重启Nginx服务器

访问B服务器的IP地址，我这里设置的B服务器IP地址是 localhost 端口是80.

就会反向代理到A服务器上， 去轮询的请求 A服务器的8001端口，8002端口，8003端口。

这就实现了Nginx的负载均衡，可以让服务器高可用性更强，处理大量的并发事件更容易些，性能更快一些。

如果您不知道访问的哪个端口，无法做明现的判断：

再A服务器上的server.conf配置项中：

 root   /opt/app/code1;   # 路径下面有对应的文件 index.html 
		index  index.html index.htm;

再/opt/app/code1/目录下，创建一个 index.html 文件

文件内容如下：

<html>
<head>
    <meta charset="utf-8">
    <title>server3</title>
</head>
<body style="background-color:green;">
    <h1>Server 3<h1>
</body>
</html>

再8002， 8003 端口都配置相同的文件，加以区分。


Upstream举例：
upstream 举例：

upstream backend {  # backend: 组的名字
    server upstream backend1.example.com weight=5;  # 可以支持ip的写法，也支持域名的方式
    server upstream backend2.example.com:8080;
    server unix:/tmp/backend3;  # 也可以支持socket 的方式来写
    
    server backup1.example.com:8080  backup;  # backup 表示是备份的节点  
    server backup2.example.com:8080  backup;  # weight 表示是权重节点，对于轮询方式而言，参数越大，轮询到的概率越大
}

三、服务器再负载均衡调度中的状态：
down	当前的server暂时不参与负载均衡
backup	预备的备份服务器
max_fails	允许请求失败的次数
fail_timeout	经过max_fails失败后，服务暂停的时间
max_conns	限制最大的接收连接数
 upstream tomcat_server {
              server 192.168.37.129:8001 down;

              server 192.168.37.129:8002 backup;

              server 192.168.37.129:8003 max_fails=1 fail_timeout=10s; 
        }

四、Nginx 提供了多种负载均衡策略
轮询	按时间顺序逐一分配到不同的后端服务器
加权轮询	weight值越大，分配到的访问几率越大
ip_hash	每个请求按访问IP的hash结果分配，这样来自同一个IP的固定访问一个后端服务器
url_hash	按照访问的URL的hash结果分配请求，是每个URL定向到同一个后端服务器
least_conn	最少链接数，那个机器链接数少就分发
hash关键数值	hash自定义的key


负载均衡策略在各种分布式系统中基本上原理一致，对于原理有兴趣，不妨参考：https://dunwu.github.io/blog/design/theory/load-balance-theory/

轮询：

upstream bck_testing_01 {
  # 默认所有服务器权重为 1
  server 192.168.250.220:8080
  server 192.168.250.221:8080
  server 192.168.250.222:8080
}
加权轮询：

upstream bck_testing_01 {
  server 192.168.250.220:8080   weight=3
  server 192.168.250.221:8080              # default weight=1
  server 192.168.250.222:8080              # default weight=1
}
最少连接：

upstream bck_testing_01 {
  least_conn;

  # with default weight for all (weight=1)
  server 192.168.250.220:8080
  server 192.168.250.221:8080
  server 192.168.250.222:8080
}
加权最少连接：

upstream bck_testing_01 {
  least_conn;

  server 192.168.250.220:8080   weight=3
  server 192.168.250.221:8080              # default weight=1
  server 192.168.250.222:8080              # default weight=1
}
IP Hash：
假如有7个请求，8002端口会接收到5个请求。
总结：加权轮询跟轮询都是基于请求来进行分配如果不想依赖请求的话，想把cookie和seesion的信息进行保存一致，要不然用户再请求页面的时候，访问不同的服务区，会导致一些重新登录的场景，可以用下面的方法ip_hash。
每个用户登入的时候，带过来的IP地址，都会绑定到一台服务器上，下次访问的时候还会访问该服务器。
缺陷：
如果客户端用代理的方式来进行访问，而不是真是的IP地址，这样ip_hash作用就没有那么大了。

upstream bck_testing_01 {

  ip_hash;

  # with default weight for all (weight=1)
  server 192.168.250.220:8080
  server 192.168.250.221:8080
  server 192.168.250.222:8080

}
普通 Hash：

upstream bck_testing_01 {

  hash $request_uri;

  # with default weight for all (weight=1)
  server 192.168.250.220:8080
  server 192.168.250.221:8080
  server 192.168.250.222:8080

}

五 网站有多个 webapp 的配置

当一个网站功能越来越丰富时，往往需要将一些功能相对独立的模块剥离出来，独立维护。这样的话，通常，会有多个 webapp。

举个例子：假如 www.helloworld.com 站点有好几个 webapp，finance（金融）、product（产品）、admin（用户中心）。访问这些应用的方式通过上下文（context）来进行区分：

www.helloworld.com/finance/

www.helloworld.com/product/

www.helloworld.com/admin/

我们知道，HTTP 的默认端口号是 80，如果在一台服务器上同时启动这 3 个 webapp 应用，都用 80 端口，肯定是不成的。所以，这三个应用需要分别绑定不同的端口号。

那么，问题来了，用户在实际访问 www.helloworld.com 站点时，访问不同 webapp，总不会还带着对应的端口号去访问吧。所以，你再次需要用到反向代理来做处理。

配置也不难，来看看怎么做吧：

http {
 #此处省略一些基本配置

 upstream product_server{
  server www.helloworld.com:8081;
 }

 upstream admin_server{
  server www.helloworld.com:8082;
 }

 upstream finance_server{
  server www.helloworld.com:8083;
 }

 server {
  #此处省略一些基本配置
  #默认指向 product 的 server
  location / {
   proxy_pass http://product_server;
  }

  location /product/{
   proxy_pass http://product_server;
  }

  location /admin/ {
   proxy_pass http://admin_server;
  }

  location /finance/ {
   proxy_pass http://finance_server;
  }
 }
}


